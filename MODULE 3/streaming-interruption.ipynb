{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d9709c",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239464-lesson-1-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f6e26b",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "## Review\n",
    "\n",
    "In module 2, covered a few ways to customize graph state and memory.\n",
    " \n",
    "We built up to a Chatbot with external memory that can sustain long-running conversations. \n",
    "\n",
    "## Goals\n",
    "\n",
    "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways. \n",
    "\n",
    "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d11343",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c92e5d",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "LangGraph is built with [first class support for streaming](https://docs.langchain.com/oss/python/langgraph/streaming).\n",
    "\n",
    "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b692a6",
   "metadata": {},
   "source": [
    "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfac718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) \n",
    "\n",
    "# State \n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State, config: RunnableConfig):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages, config)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State)-> Literal [\"summarize_conversation\",END]:\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eef9a6",
   "metadata": {},
   "source": [
    "### Streaming full state\n",
    "\n",
    "Now, let's talk about ways to [stream our graph state](https://docs.langchain.com/oss/python/langgraph/streaming#supported-stream-modes).\n",
    "\n",
    "`.stream` and `.astream` are sync and async methods for streaming back results. \n",
    " \n",
    "LangGraph supports a few [different streaming modes](https://docs.langchain.com/oss/python/langgraph/streaming#stream-graph-state) for graph state.\n",
    " \n",
    "* `values`: This streams the full state of the graph after each node is called.\n",
    "* `updates`: This streams updates to the state of the graph after each node is called.\n",
    "\n",
    "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
    "\n",
    "Let's look at `stream_mode=\"updates\"`.\n",
    "\n",
    "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
    "\n",
    "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e26a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation': {'messages': AIMessage(content='Hi Arjun! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 12, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b547601dbd', 'id': 'chatcmpl-CgVf3gh0ty2iwMGHeZbvx8JTTnN2B', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--26ec7f6c-a67d-4506-ab3d-ea9a7f5c6a3a-0', usage_metadata={'input_tokens': 12, 'output_tokens': 11, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Arjun\")]}, config, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147b23a",
   "metadata": {},
   "source": [
    "Let's now just print the state update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba320d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi again, Arjun! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Arjun\")]}, config, stream_mode=\"updates\"):\n",
    "    chunk['conversation'][\"messages\"].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034de5bc",
   "metadata": {},
   "source": [
    "Now, we can see `stream_mode=\"values\"`.\n",
    "\n",
    "This is the `full state` of the graph after the `conversation` node is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e3d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Arjun\n",
      "---------------------------------------------------------------------------\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Arjun\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Arjun! How can I assist you today?\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Arjun\")\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    for m in event['messages']:\n",
    "        m.pretty_print()\n",
    "    print(\"---\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2d1e6",
   "metadata": {},
   "source": [
    "### Streaming tokens\n",
    "\n",
    "We often want to stream more than graph state.\n",
    "\n",
    "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
    "\n",
    "We can do this [using the `.astream_events` method](https://docs.langchain.com/oss/python/langchain/models#advanced-streaming-topics:streaming-events), which streams back events as they happen inside nodes!\n",
    "\n",
    "Each event is a dict with a few keys:\n",
    " \n",
    "* `event`: This is the type of event that is being emitted. \n",
    "* `name`: This is the name of event.\n",
    "* `data`: This is the data associated with the event.\n",
    "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86d145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: . Type: on_chain_start. Name: LangGraph\n",
      "Node: conversation. Type: on_chain_start. Name: conversation\n",
      "Node: conversation. Type: on_chat_model_start. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_end. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chain_start. Name: should_continue\n",
      "Node: conversation. Type: on_chain_end. Name: should_continue\n",
      "Node: conversation. Type: on_chain_stream. Name: conversation\n",
      "Node: conversation. Type: on_chain_end. Name: conversation\n",
      "Node: . Type: on_chain_stream. Name: LangGraph\n",
      "Node: . Type: on_chain_end. Name: LangGraph\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d37803",
   "metadata": {},
   "source": [
    "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
    "\n",
    "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
    "\n",
    "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Barcelona', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' commonly', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' referred', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Bara', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' one', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' most', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' successful', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' popular', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' clubs', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' world', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Founded', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='189', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' based', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Barcelona', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Catal', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='onia', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Spain', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Here', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' some', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' key', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' points', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' about', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' FC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Barcelona', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' History', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='Foundation', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' FC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Barcelona', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' was', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' established', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' on', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' November', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='29', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='189', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' by', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' group', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Swiss', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Catal', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='an', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' German', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' English', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' led', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' by', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Joan', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Gam', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='per', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='Early', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Years', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' quickly', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' gained', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' popularity', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' its', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' first', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' trophy', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Copa', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Mac', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='aya', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='190', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='2', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Ach', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='ievements', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='Domestic', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Success', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Barcelona', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' numerous', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' La', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Liga', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' titles', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' making', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' it', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' one', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' most', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' successful', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' clubs', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Spanish', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' history', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' They', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' also', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Copa', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' del', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Rey', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' multiple', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' times', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='International', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Success', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' enjoyed', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' significant', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' success', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' European', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' competitions', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' winning', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' UEFA', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Champions', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' League', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' multiple', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' times', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' notable', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' victories', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='199', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='2', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='200', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='6', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='200', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='201', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='1', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='201', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='5', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='F', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='IFA', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' World', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Cup', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Barcelona', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' also', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' FIFA', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' World', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Cup', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' showcasing', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' dominance', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' on', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' global', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' scale', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Style', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Play', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='T', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='iki', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-T', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='aka', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' FC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Barcelona', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' renowned', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' for', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' its', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' distinctive', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' style', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' play', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' \"', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='t', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='iki', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-t', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='aka', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',\"', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' characterized', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' by', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' short', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' passing', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' movement', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' maintaining', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' possession', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' creating', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' space', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='La', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Mas', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='ia', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' youth', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' academy', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' La', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Mas', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='ia', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' produced', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' many', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' world', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-class', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' players', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' including', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Lionel', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Messi', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' X', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='avi', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Hernandez', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Andrs', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Ini', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='esta', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' contributing', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' philosophy', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' promoting', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' home', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='grown', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' talent', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Not', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='able', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Players', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='Lion', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='el', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Messi', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Wid', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='ely', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' regarded', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' one', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' greatest', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' all', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' time', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Messi', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' spent', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' over', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='20', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' years', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' becoming', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' its', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' all', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-time', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' leading', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' scorer', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' winning', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' numerous', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' individual', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' awards', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='Other', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Legends', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Other', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' notable', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' players', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' include', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Johan', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Cru', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='y', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='ff', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Ronald', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='inho', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' X', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='avi', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Ini', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='esta', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' more', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' recently', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' players', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' like', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Gerard', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' P', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='iqu', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Sergio', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Bus', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='quets', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Rival', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='ries', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='El', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Cl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='ico', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' FC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Barcelona', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' fierce', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' rivalry', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Real', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Madrid', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' El', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Cl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='ico', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Matches', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' between', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' these', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' two', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' clubs', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' among', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' most', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-w', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='atched', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' sporting', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' events', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' globally', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='Der', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='by', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' also', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' local', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' rivalry', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Esp', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='anyol', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Barcelona', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Derby', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Current', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Status', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' As', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' my', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' last', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' update', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' October', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='202', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='3', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' FC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Barcelona', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' continues', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' compete', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' highest', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' levels', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' both', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' domestic', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' European', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' competitions', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' faced', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' challenges', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' including', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' financial', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' difficulties', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' changes', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' management', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' but', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' remains', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' powerhouse', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Cultural', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Impact', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='M', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' que', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' un', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' motto', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' \"', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='M', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' que', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' un', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='\"', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='More', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' than', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='),', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' reflects', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' its', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' significance', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Catal', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='an', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' culture', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' identity', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' well', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' its', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' commitment', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' social', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' issues', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' community', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' involvement', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Barcelona', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' not', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' just', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=';', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' it', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' symbol', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' Catal', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='an', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' pride', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' global', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' brand', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' millions', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' fans', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' around', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content=' world', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb')}\n",
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_50906f2aac', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb', chunk_position='last')}\n",
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb', usage_metadata={'input_tokens': 14, 'output_tokens': 619, 'total_tokens': 633, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}\n",
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--efc5bc10-0a4f-402c-bf87-63faed2036fb', chunk_position='last')}\n"
     ]
    }
   ],
   "source": [
    "node_to_stream = 'conversation'\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the FC Barcelona team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        print(event[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90cc714",
   "metadata": {},
   "source": [
    "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b3fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|FC| Barcelona|,| commonly| referred| to| as| Bara|,| is| one| of| the| most| successful| and| popular| football| clubs| in| the| world|.| Founded| in| |189|9|,| the| club| is| based| in| Barcelona|,| Catal|onia|,| Spain|.| Here| are| some| key| points| about| FC| Barcelona|:\n",
      "\n",
      "|###| History|\n",
      "|-| **|Foundation|**|:| FC| Barcelona| was| established| by| a| group| of| Swiss|,| Catal|an|,| German|,| and| English| football|ers| led| by| Joan| Gam|per|.\n",
      "|-| **|Early| Years|**|:| The| club| quickly| gained| popularity| and| won| its| first| trophy|,| the| Copa| del| Rey|,| in| |190|2|.\n",
      "\n",
      "|###| Ach|ievements|\n",
      "|-| **|Domestic| Success|**|:| Barcelona| has| won| numerous| La| Liga| titles|,| making| it| one| of| the| most| successful| clubs| in| Spanish| football| history|.| They| have| also| won| the| Copa| del| Rey| multiple| times|.\n",
      "|-| **|International| Success|**|:| The| club| has| enjoyed| significant| success| in| European| competitions|,| winning| the| UEFA| Champions| League| multiple| times|,| with| notable| victories| in| |199|2|,| |200|6|,| |200|9|,| |201|1|,| and| |201|5|.\n",
      "|-| **|F|IFA| Club| World| Cup|**|:| Barcelona| has| also| won| the| FIFA| Club| World| Cup|,| showcasing| their| status| as| one| of| the| best| clubs| globally|.\n",
      "\n",
      "|###| Style| of| Play|\n",
      "|-| **|T|iki|-T|aka|**|:| FC| Barcelona| is| known| for| its| distinctive| style| of| play|,| often| referred| to| as| \"|t|iki|-t|aka|,\"| characterized| by| short| passing| and| movement|,| maintaining| possession|,| and| creating| space|.\n",
      "|-| **|La| Mas|ia|**|:| The| club|'s| youth| academy|,| La| Mas|ia|,| has| produced| many| world|-class| players|,| including| Lionel| Messi|,| X|avi| Hernandez|,| and| Andrs| Ini|esta|,| contributing| to| the| team's| success| over| the| years|.\n",
      "\n",
      "|###| Not|able| Players|\n",
      "|-| **|Lion|el| Messi|**|:| Wid|ely| regarded| as| one| of| the| greatest| football|ers| of| all| time|,| Messi| spent| over| |20| years| at| the| club|,| becoming| its| all|-time| leading| scorer| and| winning| numerous| individual| awards|.\n",
      "|-| **|Other| Legends|**|:| Other| notable| players| include| Johan| Cru|y|ff|,| Ronald|inho|,| X|avi|,| Ini|esta|,| and| more| recently|,| players| like| Gerard| P|iqu| and| Sergio| Bus|quets|.\n",
      "\n",
      "|###| Rival|ries|\n",
      "|-| **|El| Cl|s|ico|**|:| FC| Barcelona| has| a| fierce| rivalry| with| Real| Madrid|,| known| as| El| Cl|s|ico|.| Matches| between| these| two| clubs| are| among| the| most|-w|atched| sporting| events| globally|.\n",
      "|-| **|Der|by|**|:| The| club| also| has| a| local| rivalry| with| Esp|anyol|,| known| as| the| Barcelona| Derby|.\n",
      "\n",
      "|###| Recent| Develop|ments|\n",
      "|-| In| recent| years|,| FC| Barcelona| has| faced| challenges|,| including| financial| difficulties| and| changes| in| management|.| The| club| has| been| working| to| rebuild| its| squad| and| return| to| its| former| glory|.\n",
      "\n",
      "|###| Stadium|\n",
      "|-| **|Camp| Nou|**|:| Barcelona| plays| its| home| matches| at| Camp| Nou|,| one| of| the| largest| stadium|s| in| Europe|,| with| a| capacity| of| over| |99|,|000| spectators|.| The| stadium| is| an| iconic| symbol| of| the| club| and| has| hosted| numerous| significant| matches|,| including| the| UEFA| Champions| League| final|.\n",
      "\n",
      "|###| Culture| and| Identity|\n",
      "|-| **|M|s| que| un| club|**|:| The| club|'s| motto|,| \"|M|s| que| un| club|\"| (|More| than| a| club|),| reflects| its| deep| connection| to| Catal|an| identity| and| culture|,| as| well| as| its| commitment| to| social| issues|.\n",
      "\n",
      "|FC| Barcelona| continues| to| be| a| major| force| in| both| Spanish| and| international| football|,| with| a| rich| history| and| a| passionate| fan| base|.||||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the FC Barcelona team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        data = event[\"data\"]\n",
    "        print(data[\"chunk\"].content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68108e7f",
   "metadata": {},
   "source": [
    "### Streaming with LangGraph API\n",
    "\n",
    "** Notice**\n",
    "\n",
    "Since filming these videos, we've updated Studio so that it can now be run locally and accessed through your browser. This is the preferred way to run Studio instead of using the Desktop App shown in the video. It is now called _LangSmith Studio_ instead of _LangGraph Studio_. Detailed setup instructions are available in the \"Getting Setup\" guide at the start of the course. You can find a description of Studio [here](https://docs.langchain.com/langsmith/studio), and specific details for local deployment [here](https://docs.langchain.com/langsmith/quick-start-studio#local-development-server).  \n",
    "To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "-  API: http://127.0.0.1:2024\n",
    "-  Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "-  API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the **Studio UI** URL shown above.\n",
    "\n",
    "The LangGraph API  [supports editing graph state](https://docs.langchain.com/langsmith/add-human-in-the-loop). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eace56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875cc6c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectError",
     "evalue": "All connection attempts failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n",
      "\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n",
      "\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n",
      "\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n",
      "\u001b[32m    237\u001b[39m         pool_request.request\n",
      "\u001b[32m    238\u001b[39m     )\n",
      "\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n",
      "\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n",
      "\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n",
      "\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_async/connection.py:101\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_async/connection.py:78\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect(request)\n",
      "\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_async/connection.py:124\u001b[39m, in \u001b[36mAsyncHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    123\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_backend.connect_tcp(**kwargs)\n",
      "\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_backends/auto.py:31\u001b[39m, in \u001b[36mAutoBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n",
      "\u001b[32m     30\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_backend()\n",
      "\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.connect_tcp(\n",
      "\u001b[32m     32\u001b[39m     host,\n",
      "\u001b[32m     33\u001b[39m     port,\n",
      "\u001b[32m     34\u001b[39m     timeout=timeout,\n",
      "\u001b[32m     35\u001b[39m     local_address=local_address,\n",
      "\u001b[32m     36\u001b[39m     socket_options=socket_options,\n",
      "\u001b[32m     37\u001b[39m )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_backends/anyio.py:113\u001b[39m, in \u001b[36mAnyIOBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n",
      "\u001b[32m    108\u001b[39m exc_map = {\n",
      "\u001b[32m    109\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ConnectTimeout,\n",
      "\u001b[32m    110\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n",
      "\u001b[32m    111\u001b[39m     anyio.BrokenResourceError: ConnectError,\n",
      "\u001b[32m    112\u001b[39m }\n",
      "\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfail_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n",
      "\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n",
      "\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n",
      "\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n",
      "\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n",
      "\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n",
      "\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n",
      "\u001b[32m      5\u001b[39m client = get_client(url=URL)\n",
      "\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Search all hosted graphs\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m assistants = \u001b[38;5;28;01mawait\u001b[39;00m client.assistants.search()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/langgraph_sdk/client.py:1091\u001b[39m, in \u001b[36mAssistantsClient.search\u001b[39m\u001b[34m(self, metadata, graph_id, name, limit, offset, sort_by, sort_order, select, headers, params)\u001b[39m\n",
      "\u001b[32m   1089\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m select:\n",
      "\u001b[32m   1090\u001b[39m     payload[\u001b[33m\"\u001b[39m\u001b[33mselect\u001b[39m\u001b[33m\"\u001b[39m] = select\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.http.post(\n",
      "\u001b[32m   1092\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/assistants/search\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m   1093\u001b[39m     json=payload,\n",
      "\u001b[32m   1094\u001b[39m     headers=headers,\n",
      "\u001b[32m   1095\u001b[39m     params=params,\n",
      "\u001b[32m   1096\u001b[39m )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/langgraph_sdk/client.py:343\u001b[39m, in \u001b[36mHttpClient.post\u001b[39m\u001b[34m(self, path, json, params, headers, on_response)\u001b[39m\n",
      "\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m headers:\n",
      "\u001b[32m    342\u001b[39m     request_headers.update(headers)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m r = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.post(\n",
      "\u001b[32m    344\u001b[39m     path, headers=request_headers, content=content, params=params\n",
      "\u001b[32m    345\u001b[39m )\n",
      "\u001b[32m    346\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_response:\n",
      "\u001b[32m    347\u001b[39m     on_response(r)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_client.py:1859\u001b[39m, in \u001b[36mAsyncClient.post\u001b[39m\u001b[34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n",
      "\u001b[32m   1838\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n",
      "\u001b[32m   1839\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[32m   1840\u001b[39m     url: URL | \u001b[38;5;28mstr\u001b[39m,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   1852\u001b[39m     extensions: RequestExtensions | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[32m   1853\u001b[39m ) -> Response:\n",
      "\u001b[32m   1854\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[32m   1855\u001b[39m \u001b[33;03m    Send a `POST` request.\u001b[39;00m\n",
      "\u001b[32m   1856\u001b[39m \n",
      "\u001b[32m   1857\u001b[39m \u001b[33;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n",
      "\u001b[32m   1858\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(\n",
      "\u001b[32m   1860\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m   1861\u001b[39m         url,\n",
      "\u001b[32m   1862\u001b[39m         content=content,\n",
      "\u001b[32m   1863\u001b[39m         data=data,\n",
      "\u001b[32m   1864\u001b[39m         files=files,\n",
      "\u001b[32m   1865\u001b[39m         json=json,\n",
      "\u001b[32m   1866\u001b[39m         params=params,\n",
      "\u001b[32m   1867\u001b[39m         headers=headers,\n",
      "\u001b[32m   1868\u001b[39m         cookies=cookies,\n",
      "\u001b[32m   1869\u001b[39m         auth=auth,\n",
      "\u001b[32m   1870\u001b[39m         follow_redirects=follow_redirects,\n",
      "\u001b[32m   1871\u001b[39m         timeout=timeout,\n",
      "\u001b[32m   1872\u001b[39m         extensions=extensions,\n",
      "\u001b[32m   1873\u001b[39m     )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_client.py:1540\u001b[39m, in \u001b[36mAsyncClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n",
      "\u001b[32m   1525\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n",
      "\u001b[32m   1527\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n",
      "\u001b[32m   1528\u001b[39m     method=method,\n",
      "\u001b[32m   1529\u001b[39m     url=url,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   1538\u001b[39m     extensions=extensions,\n",
      "\u001b[32m   1539\u001b[39m )\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n",
      "\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n",
      "\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n",
      "\u001b[32m   1630\u001b[39m     request,\n",
      "\u001b[32m   1631\u001b[39m     auth=auth,\n",
      "\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n",
      "\u001b[32m   1633\u001b[39m     history=[],\n",
      "\u001b[32m   1634\u001b[39m )\n",
      "\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n",
      "\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n",
      "\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n",
      "\u001b[32m   1658\u001b[39m         request,\n",
      "\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n",
      "\u001b[32m   1660\u001b[39m         history=history,\n",
      "\u001b[32m   1661\u001b[39m     )\n",
      "\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n",
      "\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n",
      "\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   1727\u001b[39m     )\n",
      "\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n",
      "\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n",
      "\u001b[32m   1733\u001b[39m response.request = request\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_transports/default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    379\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n",
      "\u001b[32m    381\u001b[39m req = httpcore.Request(\n",
      "\u001b[32m    382\u001b[39m     method=request.method,\n",
      "\u001b[32m    383\u001b[39m     url=httpcore.URL(\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n",
      "\u001b[32m    392\u001b[39m )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_async_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n",
      "\u001b[32m    156\u001b[39m     value = typ()\n",
      "\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n",
      "\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n",
      "\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n",
      "\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed"
     ]
    }
   ],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "# This is the URL of the local development server\n",
    "URL = \"http://127.0.0.1:2024\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcdcff1",
   "metadata": {},
   "source": [
    "Let's [stream `values`](https://docs.langchain.com/oss/python/langgraph/streaming#stream-graph-state), like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd613aed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectError",
     "evalue": "All connection attempts failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n",
      "\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n",
      "\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n",
      "\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n",
      "\u001b[32m    237\u001b[39m         pool_request.request\n",
      "\u001b[32m    238\u001b[39m     )\n",
      "\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n",
      "\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n",
      "\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n",
      "\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_async/connection.py:101\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_async/connection.py:78\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect(request)\n",
      "\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_async/connection.py:124\u001b[39m, in \u001b[36mAsyncHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    123\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_backend.connect_tcp(**kwargs)\n",
      "\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_backends/auto.py:31\u001b[39m, in \u001b[36mAutoBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n",
      "\u001b[32m     30\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_backend()\n",
      "\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.connect_tcp(\n",
      "\u001b[32m     32\u001b[39m     host,\n",
      "\u001b[32m     33\u001b[39m     port,\n",
      "\u001b[32m     34\u001b[39m     timeout=timeout,\n",
      "\u001b[32m     35\u001b[39m     local_address=local_address,\n",
      "\u001b[32m     36\u001b[39m     socket_options=socket_options,\n",
      "\u001b[32m     37\u001b[39m )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_backends/anyio.py:113\u001b[39m, in \u001b[36mAnyIOBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n",
      "\u001b[32m    108\u001b[39m exc_map = {\n",
      "\u001b[32m    109\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ConnectTimeout,\n",
      "\u001b[32m    110\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n",
      "\u001b[32m    111\u001b[39m     anyio.BrokenResourceError: ConnectError,\n",
      "\u001b[32m    112\u001b[39m }\n",
      "\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfail_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n",
      "\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n",
      "\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n",
      "\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n",
      "\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n",
      "\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n",
      "\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create a new thread\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m thread = \u001b[38;5;28;01mawait\u001b[39;00m client.threads.create()\n",
      "\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Input message\u001b[39;00m\n",
      "\u001b[32m      4\u001b[39m input_message = HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mDivide 9 by 4\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/langgraph_sdk/client.py:1348\u001b[39m, in \u001b[36mThreadsClient.create\u001b[39m\u001b[34m(self, metadata, thread_id, if_exists, supersteps, graph_id, ttl, headers, params)\u001b[39m\n",
      "\u001b[32m   1345\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m   1346\u001b[39m         payload[\u001b[33m\"\u001b[39m\u001b[33mttl\u001b[39m\u001b[33m\"\u001b[39m] = ttl\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1348\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.http.post(\n",
      "\u001b[32m   1349\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/threads\u001b[39m\u001b[33m\"\u001b[39m, json=payload, headers=headers, params=params\n",
      "\u001b[32m   1350\u001b[39m )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/langgraph_sdk/client.py:343\u001b[39m, in \u001b[36mHttpClient.post\u001b[39m\u001b[34m(self, path, json, params, headers, on_response)\u001b[39m\n",
      "\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m headers:\n",
      "\u001b[32m    342\u001b[39m     request_headers.update(headers)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m r = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.post(\n",
      "\u001b[32m    344\u001b[39m     path, headers=request_headers, content=content, params=params\n",
      "\u001b[32m    345\u001b[39m )\n",
      "\u001b[32m    346\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_response:\n",
      "\u001b[32m    347\u001b[39m     on_response(r)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_client.py:1859\u001b[39m, in \u001b[36mAsyncClient.post\u001b[39m\u001b[34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n",
      "\u001b[32m   1838\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n",
      "\u001b[32m   1839\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[32m   1840\u001b[39m     url: URL | \u001b[38;5;28mstr\u001b[39m,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   1852\u001b[39m     extensions: RequestExtensions | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[32m   1853\u001b[39m ) -> Response:\n",
      "\u001b[32m   1854\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[32m   1855\u001b[39m \u001b[33;03m    Send a `POST` request.\u001b[39;00m\n",
      "\u001b[32m   1856\u001b[39m \n",
      "\u001b[32m   1857\u001b[39m \u001b[33;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n",
      "\u001b[32m   1858\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(\n",
      "\u001b[32m   1860\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m   1861\u001b[39m         url,\n",
      "\u001b[32m   1862\u001b[39m         content=content,\n",
      "\u001b[32m   1863\u001b[39m         data=data,\n",
      "\u001b[32m   1864\u001b[39m         files=files,\n",
      "\u001b[32m   1865\u001b[39m         json=json,\n",
      "\u001b[32m   1866\u001b[39m         params=params,\n",
      "\u001b[32m   1867\u001b[39m         headers=headers,\n",
      "\u001b[32m   1868\u001b[39m         cookies=cookies,\n",
      "\u001b[32m   1869\u001b[39m         auth=auth,\n",
      "\u001b[32m   1870\u001b[39m         follow_redirects=follow_redirects,\n",
      "\u001b[32m   1871\u001b[39m         timeout=timeout,\n",
      "\u001b[32m   1872\u001b[39m         extensions=extensions,\n",
      "\u001b[32m   1873\u001b[39m     )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_client.py:1540\u001b[39m, in \u001b[36mAsyncClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n",
      "\u001b[32m   1525\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n",
      "\u001b[32m   1527\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n",
      "\u001b[32m   1528\u001b[39m     method=method,\n",
      "\u001b[32m   1529\u001b[39m     url=url,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   1538\u001b[39m     extensions=extensions,\n",
      "\u001b[32m   1539\u001b[39m )\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n",
      "\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n",
      "\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n",
      "\u001b[32m   1630\u001b[39m     request,\n",
      "\u001b[32m   1631\u001b[39m     auth=auth,\n",
      "\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n",
      "\u001b[32m   1633\u001b[39m     history=[],\n",
      "\u001b[32m   1634\u001b[39m )\n",
      "\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n",
      "\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n",
      "\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n",
      "\u001b[32m   1658\u001b[39m         request,\n",
      "\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n",
      "\u001b[32m   1660\u001b[39m         history=history,\n",
      "\u001b[32m   1661\u001b[39m     )\n",
      "\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n",
      "\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n",
      "\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   1727\u001b[39m     )\n",
      "\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n",
      "\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n",
      "\u001b[32m   1733\u001b[39m response.request = request\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_transports/default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    379\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n",
      "\u001b[32m    381\u001b[39m req = httpcore.Request(\n",
      "\u001b[32m    382\u001b[39m     method=request.method,\n",
      "\u001b[32m    383\u001b[39m     url=httpcore.URL(\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n",
      "\u001b[32m    392\u001b[39m )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_async_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n",
      "\u001b[32m    156\u001b[39m     value = typ()\n",
      "\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n",
      "\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n",
      "\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n",
      "\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed"
     ]
    }
   ],
   "source": [
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "# Input message\n",
    "input_message = HumanMessage(content=\"Divide 9 by 4\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a316e",
   "metadata": {},
   "source": [
    "The streamed objects have: \n",
    "\n",
    "* `event`: Type\n",
    "* `data`: State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "content='Multiply 2 and 3' additional_kwargs={} response_metadata={} id='c3ec872a-99a1-4eec-bcb6-a04973f48ac5'\n",
      "=========================\n",
      "content='' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 17, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 134, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f64f290af2', 'id': 'chatcmpl-CSqw6HYoyCI7z2AuKAAfSTQGbvzla', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--c91028e7-7a0a-4746-a4f5-edcff5380abc-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_AFChrxIQGbr7mmzr8buxymY0', 'type': 'tool_call'}]\n",
      "=========================\n",
      "content='6' name='multiply' id='f69a844b-5f82-4256-96dd-92b044e888d9' tool_call_id='call_AFChrxIQGbr7mmzr8buxymY0'\n",
      "=========================\n",
      "content='The result of multiplying 2 and 3 is 6.' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 159, 'output_tokens': 14, 'total_tokens': 173, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 159, 'total_tokens': 173, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb3c3cb84d', 'id': 'chatcmpl-CSqw7xBeinGuHlx0upkmo2tryppco', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--dba1c4af-ed8f-4fed-8770-2c3429603cd0-0'\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    messages = event.data.get('messages',None)\n",
    "    if messages:\n",
    "        print(convert_to_messages(messages)[-1])\n",
    "    print('='*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7910189d",
   "metadata": {},
   "source": [
    "There are some new streaming mode that are only supported via the API.\n",
    "\n",
    "For example, we can  [use `messages` mode](https://docs.langchain.com/oss/python/langgraph/streaming#supported-stream-modes) to better handle the above case!\n",
    "\n",
    "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
    "\n",
    "All events emitted using `messages` mode have two attributes:\n",
    "\n",
    "* `event`: This is the name of the event\n",
    "* `data`: This is data associated with the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"messages\"):\n",
    "    print(event.event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8fd79c",
   "metadata": {},
   "source": [
    "We can see a few events: \n",
    "\n",
    "* `metadata`: metadata about the run\n",
    "* `messages/complete`: fully formed message \n",
    "* `messages/partial`: chat model tokens\n",
    "\n",
    "<!--You can dig further into the types [~here~](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages) [here](https://docs.langchain.com/oss/python/langgraph/concepts/langgraph_server). -->\n",
    "\n",
    "Now, let's show how to stream these messages. \n",
    "\n",
    "We'll define a helper function for better formatting of the tool calls in messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759dee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Run ID - 019a0358-57dc-76f9-bc63-633eee467a86\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "AI: The\n",
      "--------------------------------------------------\n",
      "AI: The result\n",
      "--------------------------------------------------\n",
      "AI: The result of\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "Response Metadata: Finish Reason - stop\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "Response Metadata: Finish Reason - stop\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "Response Metadata: Finish Reason - stop\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "\n",
    "def format_tool_calls(tool_calls):\n",
    "    \"\"\"\n",
    "    Format a list of tool calls into a readable string.\n",
    "\n",
    "    Args:\n",
    "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
    "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tool_calls:\n",
    "        formatted_calls = []\n",
    "        for call in tool_calls:\n",
    "            formatted_calls.append(\n",
    "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
    "            )\n",
    "        return \"\\n\".join(formatted_calls)\n",
    "    return \"No tool calls\"\n",
    "\n",
    "async for event in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input={\"messages\": [input_message]},\n",
    "    stream_mode=\"messages\",):\n",
    "    \n",
    "    # Handle metadata events\n",
    "    if event.event == \"metadata\":\n",
    "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Handle partial message events\n",
    "    elif event.event == \"messages/partial\":\n",
    "        for data_item in event.data:\n",
    "            # Process user messages\n",
    "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
    "                print(f\"Human: {data_item['content']}\")\n",
    "            else:\n",
    "                # Extract relevant data from the event\n",
    "                tool_calls = data_item.get(\"tool_calls\", [])\n",
    "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
    "                content = data_item.get(\"content\", \"\")\n",
    "                response_metadata = data_item.get(\"response_metadata\", {})\n",
    "\n",
    "                if content:\n",
    "                    print(f\"AI: {content}\")\n",
    "\n",
    "                if tool_calls:\n",
    "                    print(\"Tool Calls:\")\n",
    "                    print(format_tool_calls(tool_calls))\n",
    "\n",
    "                if invalid_tool_calls:\n",
    "                    print(\"Invalid Tool Calls:\")\n",
    "                    print(format_tool_calls(invalid_tool_calls))\n",
    "\n",
    "                if response_metadata and response_metadata.get(\"finish_reason\"):\n",
    "                    print(f\"Response Metadata: Finish Reason - {response_metadata['finish_reason']}\")                    \n",
    "        print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
